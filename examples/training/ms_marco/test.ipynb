{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import gzip\n",
    "import json\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import tarfile\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import torch.cuda\n",
    "import tqdm\n",
    "from sentence_transformers import SentenceTransformer, LoggingHandler, util, models, losses, InputExample, evaluation\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from accelerate import Accelerator\n",
    "from accelerate import DistributedDataParallelKwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"EleutherAI/gpt-neo-1.3B\"\n",
    "cache_folder=\"/data/private/huxiaomeng/pretrained_models/\"\n",
    "max_seq_length=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function tqdm.__del__ at 0x7f294cb44a70>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/huxiaomeng/anaconda3/envs/sgpt/lib/python3.7/site-packages/tqdm/std.py\", line 1145, in __del__\n",
      "    self.close()\n",
      "  File \"/home/huxiaomeng/anaconda3/envs/sgpt/lib/python3.7/site-packages/tqdm/notebook.py\", line 283, in close\n",
      "    self.disp(bar_style='danger', check_delay=False)\n",
      "AttributeError: 'tqdm' object has no attribute 'disp'\n",
      "Some weights of GPTNeoModel were not initialized from the model checkpoint at EleutherAI/gpt-neo-1.3B and are newly initialized: ['transformer.h.1.attn.attention.bias', 'transformer.h.11.attn.attention.bias', 'transformer.h.17.attn.attention.bias', 'transformer.h.23.attn.attention.bias', 'transformer.h.21.attn.attention.bias', 'transformer.h.5.attn.attention.bias', 'transformer.h.15.attn.attention.bias', 'transformer.h.13.attn.attention.bias', 'transformer.h.7.attn.attention.bias', 'transformer.h.9.attn.attention.bias', 'transformer.h.3.attn.attention.bias', 'transformer.h.19.attn.attention.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Maximum Sequence Length:  300\n"
     ]
    }
   ],
   "source": [
    "word_embedding_model = models.Transformer(model_name, max_seq_length=max_seq_length,cache_folder=cache_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embedding_model.tokenizer.pad_token = word_embedding_model.tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [\"[SOS]\", \"{SOS}\"]\n",
    "word_embedding_model.tokenizer.add_tokens(tokens, special_tokens=True)\n",
    "word_embedding_model.auto_model.resize_token_embeddings(len(word_embedding_model.tokenizer))\n",
    "\n",
    "# Will be replaced with the rep tokens in the model ones\n",
    "# The problem is we don't know if a text is query or document when tokenizing in the Transformer.py module, \n",
    "# so we use the SOS tokens as an identifier if we have a query or document at hand & then replace them\n",
    "# If we would directly use the brackets here, they may become part of another token\n",
    "word_embedding_model.bos_spec_token_q = word_embedding_model.tokenizer.encode(\"[SOS]\", add_special_tokens=False)[0]\n",
    "word_embedding_model.bos_spec_token_d = word_embedding_model.tokenizer.encode(\"{SOS}\", add_special_tokens=False)[0]\n",
    "\n",
    "word_embedding_model.bos_spec_token_q_rep = word_embedding_model.tokenizer.encode(\"[\", add_special_tokens=False)[0]\n",
    "word_embedding_model.eos_spec_token_q = word_embedding_model.tokenizer.encode(\"]\", add_special_tokens=False)[0]\n",
    "\n",
    "word_embedding_model.bos_spec_token_d_rep = word_embedding_model.tokenizer.encode(\"{\", add_special_tokens=False)[0]\n",
    "word_embedding_model.eos_spec_token_d = word_embedding_model.tokenizer.encode(\"}\", add_special_tokens=False)[0]\n",
    "\n",
    "word_embedding_model.replace_bos = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(), \"weightedmean\")\n",
    "model = SentenceTransformer(cache_folder=cache_folder,modules=[word_embedding_model, pooling_model])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embedding_model.tokenizer.encode(\"[SOS]\", add_special_tokens=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_embedding_model.tokenizer.encode(\"[\", add_special_tokens=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Maximum Sequence Length:  300\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer(model_name_or_path=model_name,cache_folder=cache_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "a=[1,2,3]\n",
    "b=[6,7,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in zip(a,b):\n",
    "    i+=1\n",
    "    j+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 7, 8]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n"
     ]
    }
   ],
   "source": [
    "print(model._first_module().eos_spec_token_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    " import torch\n",
    " a=torch.tensor([-1,5,-9,-10,52,50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_values, top_k_idx = torch.topk(a, 3, dim=0, largest=True, sorted=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(52)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[top_k_idx[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[52, 50, 5]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_values.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 5, 3],\n",
       "        [4, 5, 1]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 5, 3],\n",
       "        [4, 5, 1]])"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_k_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9, 1, -1]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([1,9,-1],reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 6, 9, 7])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.auto_model.wte.weight\n",
      "0.auto_model.wpe.weight\n",
      "0.auto_model.h.0.ln_1.weight\n",
      "0.auto_model.h.0.ln_1.bias\n",
      "0.auto_model.h.0.attn.attention.k_proj.weight\n",
      "0.auto_model.h.0.attn.attention.v_proj.weight\n",
      "0.auto_model.h.0.attn.attention.q_proj.weight\n",
      "0.auto_model.h.0.attn.attention.out_proj.weight\n",
      "0.auto_model.h.0.attn.attention.out_proj.bias\n",
      "0.auto_model.h.0.ln_2.weight\n",
      "0.auto_model.h.0.ln_2.bias\n",
      "0.auto_model.h.0.mlp.c_fc.weight\n",
      "0.auto_model.h.0.mlp.c_fc.bias\n",
      "0.auto_model.h.0.mlp.c_proj.weight\n",
      "0.auto_model.h.0.mlp.c_proj.bias\n",
      "0.auto_model.h.1.ln_1.weight\n",
      "0.auto_model.h.1.ln_1.bias\n",
      "0.auto_model.h.1.attn.attention.k_proj.weight\n",
      "0.auto_model.h.1.attn.attention.v_proj.weight\n",
      "0.auto_model.h.1.attn.attention.q_proj.weight\n",
      "0.auto_model.h.1.attn.attention.out_proj.weight\n",
      "0.auto_model.h.1.attn.attention.out_proj.bias\n",
      "0.auto_model.h.1.ln_2.weight\n",
      "0.auto_model.h.1.ln_2.bias\n",
      "0.auto_model.h.1.mlp.c_fc.weight\n",
      "0.auto_model.h.1.mlp.c_fc.bias\n",
      "0.auto_model.h.1.mlp.c_proj.weight\n",
      "0.auto_model.h.1.mlp.c_proj.bias\n",
      "0.auto_model.h.2.ln_1.weight\n",
      "0.auto_model.h.2.ln_1.bias\n",
      "0.auto_model.h.2.attn.attention.k_proj.weight\n",
      "0.auto_model.h.2.attn.attention.v_proj.weight\n",
      "0.auto_model.h.2.attn.attention.q_proj.weight\n",
      "0.auto_model.h.2.attn.attention.out_proj.weight\n",
      "0.auto_model.h.2.attn.attention.out_proj.bias\n",
      "0.auto_model.h.2.ln_2.weight\n",
      "0.auto_model.h.2.ln_2.bias\n",
      "0.auto_model.h.2.mlp.c_fc.weight\n",
      "0.auto_model.h.2.mlp.c_fc.bias\n",
      "0.auto_model.h.2.mlp.c_proj.weight\n",
      "0.auto_model.h.2.mlp.c_proj.bias\n",
      "0.auto_model.h.3.ln_1.weight\n",
      "0.auto_model.h.3.ln_1.bias\n",
      "0.auto_model.h.3.attn.attention.k_proj.weight\n",
      "0.auto_model.h.3.attn.attention.v_proj.weight\n",
      "0.auto_model.h.3.attn.attention.q_proj.weight\n",
      "0.auto_model.h.3.attn.attention.out_proj.weight\n",
      "0.auto_model.h.3.attn.attention.out_proj.bias\n",
      "0.auto_model.h.3.ln_2.weight\n",
      "0.auto_model.h.3.ln_2.bias\n",
      "0.auto_model.h.3.mlp.c_fc.weight\n",
      "0.auto_model.h.3.mlp.c_fc.bias\n",
      "0.auto_model.h.3.mlp.c_proj.weight\n",
      "0.auto_model.h.3.mlp.c_proj.bias\n",
      "0.auto_model.h.4.ln_1.weight\n",
      "0.auto_model.h.4.ln_1.bias\n",
      "0.auto_model.h.4.attn.attention.k_proj.weight\n",
      "0.auto_model.h.4.attn.attention.v_proj.weight\n",
      "0.auto_model.h.4.attn.attention.q_proj.weight\n",
      "0.auto_model.h.4.attn.attention.out_proj.weight\n",
      "0.auto_model.h.4.attn.attention.out_proj.bias\n",
      "0.auto_model.h.4.ln_2.weight\n",
      "0.auto_model.h.4.ln_2.bias\n",
      "0.auto_model.h.4.mlp.c_fc.weight\n",
      "0.auto_model.h.4.mlp.c_fc.bias\n",
      "0.auto_model.h.4.mlp.c_proj.weight\n",
      "0.auto_model.h.4.mlp.c_proj.bias\n",
      "0.auto_model.h.5.ln_1.weight\n",
      "0.auto_model.h.5.ln_1.bias\n",
      "0.auto_model.h.5.attn.attention.k_proj.weight\n",
      "0.auto_model.h.5.attn.attention.v_proj.weight\n",
      "0.auto_model.h.5.attn.attention.q_proj.weight\n",
      "0.auto_model.h.5.attn.attention.out_proj.weight\n",
      "0.auto_model.h.5.attn.attention.out_proj.bias\n",
      "0.auto_model.h.5.ln_2.weight\n",
      "0.auto_model.h.5.ln_2.bias\n",
      "0.auto_model.h.5.mlp.c_fc.weight\n",
      "0.auto_model.h.5.mlp.c_fc.bias\n",
      "0.auto_model.h.5.mlp.c_proj.weight\n",
      "0.auto_model.h.5.mlp.c_proj.bias\n",
      "0.auto_model.h.6.ln_1.weight\n",
      "0.auto_model.h.6.ln_1.bias\n",
      "0.auto_model.h.6.attn.attention.k_proj.weight\n",
      "0.auto_model.h.6.attn.attention.v_proj.weight\n",
      "0.auto_model.h.6.attn.attention.q_proj.weight\n",
      "0.auto_model.h.6.attn.attention.out_proj.weight\n",
      "0.auto_model.h.6.attn.attention.out_proj.bias\n",
      "0.auto_model.h.6.ln_2.weight\n",
      "0.auto_model.h.6.ln_2.bias\n",
      "0.auto_model.h.6.mlp.c_fc.weight\n",
      "0.auto_model.h.6.mlp.c_fc.bias\n",
      "0.auto_model.h.6.mlp.c_proj.weight\n",
      "0.auto_model.h.6.mlp.c_proj.bias\n",
      "0.auto_model.h.7.ln_1.weight\n",
      "0.auto_model.h.7.ln_1.bias\n",
      "0.auto_model.h.7.attn.attention.k_proj.weight\n",
      "0.auto_model.h.7.attn.attention.v_proj.weight\n",
      "0.auto_model.h.7.attn.attention.q_proj.weight\n",
      "0.auto_model.h.7.attn.attention.out_proj.weight\n",
      "0.auto_model.h.7.attn.attention.out_proj.bias\n",
      "0.auto_model.h.7.ln_2.weight\n",
      "0.auto_model.h.7.ln_2.bias\n",
      "0.auto_model.h.7.mlp.c_fc.weight\n",
      "0.auto_model.h.7.mlp.c_fc.bias\n",
      "0.auto_model.h.7.mlp.c_proj.weight\n",
      "0.auto_model.h.7.mlp.c_proj.bias\n",
      "0.auto_model.h.8.ln_1.weight\n",
      "0.auto_model.h.8.ln_1.bias\n",
      "0.auto_model.h.8.attn.attention.k_proj.weight\n",
      "0.auto_model.h.8.attn.attention.v_proj.weight\n",
      "0.auto_model.h.8.attn.attention.q_proj.weight\n",
      "0.auto_model.h.8.attn.attention.out_proj.weight\n",
      "0.auto_model.h.8.attn.attention.out_proj.bias\n",
      "0.auto_model.h.8.ln_2.weight\n",
      "0.auto_model.h.8.ln_2.bias\n",
      "0.auto_model.h.8.mlp.c_fc.weight\n",
      "0.auto_model.h.8.mlp.c_fc.bias\n",
      "0.auto_model.h.8.mlp.c_proj.weight\n",
      "0.auto_model.h.8.mlp.c_proj.bias\n",
      "0.auto_model.h.9.ln_1.weight\n",
      "0.auto_model.h.9.ln_1.bias\n",
      "0.auto_model.h.9.attn.attention.k_proj.weight\n",
      "0.auto_model.h.9.attn.attention.v_proj.weight\n",
      "0.auto_model.h.9.attn.attention.q_proj.weight\n",
      "0.auto_model.h.9.attn.attention.out_proj.weight\n",
      "0.auto_model.h.9.attn.attention.out_proj.bias\n",
      "0.auto_model.h.9.ln_2.weight\n",
      "0.auto_model.h.9.ln_2.bias\n",
      "0.auto_model.h.9.mlp.c_fc.weight\n",
      "0.auto_model.h.9.mlp.c_fc.bias\n",
      "0.auto_model.h.9.mlp.c_proj.weight\n",
      "0.auto_model.h.9.mlp.c_proj.bias\n",
      "0.auto_model.h.10.ln_1.weight\n",
      "0.auto_model.h.10.ln_1.bias\n",
      "0.auto_model.h.10.attn.attention.k_proj.weight\n",
      "0.auto_model.h.10.attn.attention.v_proj.weight\n",
      "0.auto_model.h.10.attn.attention.q_proj.weight\n",
      "0.auto_model.h.10.attn.attention.out_proj.weight\n",
      "0.auto_model.h.10.attn.attention.out_proj.bias\n",
      "0.auto_model.h.10.ln_2.weight\n",
      "0.auto_model.h.10.ln_2.bias\n",
      "0.auto_model.h.10.mlp.c_fc.weight\n",
      "0.auto_model.h.10.mlp.c_fc.bias\n",
      "0.auto_model.h.10.mlp.c_proj.weight\n",
      "0.auto_model.h.10.mlp.c_proj.bias\n",
      "0.auto_model.h.11.ln_1.weight\n",
      "0.auto_model.h.11.ln_1.bias\n",
      "0.auto_model.h.11.attn.attention.k_proj.weight\n",
      "0.auto_model.h.11.attn.attention.v_proj.weight\n",
      "0.auto_model.h.11.attn.attention.q_proj.weight\n",
      "0.auto_model.h.11.attn.attention.out_proj.weight\n",
      "0.auto_model.h.11.attn.attention.out_proj.bias\n",
      "0.auto_model.h.11.ln_2.weight\n",
      "0.auto_model.h.11.ln_2.bias\n",
      "0.auto_model.h.11.mlp.c_fc.weight\n",
      "0.auto_model.h.11.mlp.c_fc.bias\n",
      "0.auto_model.h.11.mlp.c_proj.weight\n",
      "0.auto_model.h.11.mlp.c_proj.bias\n",
      "0.auto_model.ln_f.weight\n",
      "0.auto_model.ln_f.bias\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)\n",
    "    if \"bias\" in name:\n",
    "        # Freeze all except bias\n",
    "        continue \n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=[\"[SOS] I love you\",\"[SOS] Do you love me\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=model._first_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.bos_spec_token_q\n",
    "a.bos_spec_token_q_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 4\n",
      "2 5\n",
      "3 6\n"
     ]
    }
   ],
   "source": [
    "a=[1,2,3]\n",
    "b=[4,5,6]\n",
    "out={'a':a,'b':b}\n",
    "newa=[]\n",
    "newb=[]\n",
    "for i,j in zip(out['a'],out['b']):\n",
    "    print(i,j)\n",
    "    i=5\n",
    "    j=5\n",
    "    newa.append(i)\n",
    "    newb.append(j)\n",
    "out['a']=newa\n",
    "out['b']=newb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[5, 5, 5]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['b']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   58,   314,  1842,   345,    60, 50256],\n",
       "         [   58,  2141,   345,  1842,   502,    60]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 0],\n",
       "         [1, 1, 1, 1, 1, 1]])}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [\"[SOS]\", \"{SOS}\"]\n",
    "model._first_module().tokenizer.add_tokens(tokens, special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "_batch_encode_plus() got an unexpected keyword argument 'add_soecial_tokens'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1550992/2667571385.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{SOS}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0madd_soecial_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/sgpt/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, return_tensors, **kwargs)\u001b[0m\n\u001b[1;32m   2233\u001b[0m             \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2234\u001b[0m             \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_tensors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2235\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2236\u001b[0m         )\n\u001b[1;32m   2237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sgpt/lib/python3.7/site-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36mencode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2572\u001b[0m             \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2573\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2574\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2575\u001b[0m         )\n\u001b[1;32m   2576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sgpt/lib/python3.7/site-packages/transformers/models/gpt2/tokenization_gpt2_fast.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m         )\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 174\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_encode_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msave_vocabulary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_directory\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename_prefix\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sgpt/lib/python3.7/site-packages/transformers/tokenization_utils_fast.py\u001b[0m in \u001b[0;36m_encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    514\u001b[0m             \u001b[0mreturn_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m         )\n\u001b[1;32m    518\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/sgpt/lib/python3.7/site-packages/transformers/models/gpt2/tokenization_gpt2_fast.py\u001b[0m in \u001b[0;36m_batch_encode_plus\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m         )\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_encode_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_encode_plus\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mBatchEncoding\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: _batch_encode_plus() got an unexpected keyword argument 'add_soecial_tokens'"
     ]
    }
   ],
   "source": [
    "model._first_module().tokenizer.encode(\"{SOS}\",add_soecial_tokens=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='/data/private/huxiaomeng/sgpt/checkpoints/sgpt-1.3G/15599/', vocab_size=50257, model_max_len=2048, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|endoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '<|endoftext|>'})"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model._first_module().tokenizer"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "03ae275a2001a585003f092f23e1124dfb473821afe59c38485a3c5908e9550c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('sgpt': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
